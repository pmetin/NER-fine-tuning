{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjeS9rt5Flfa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel, EarlyStoppingCallback\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "ysLcRlIbGA9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project :\n",
        "\n"
      ],
      "metadata": {
        "id": "8CAXCfi875X3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was created to fine-tune a pre-trained model for named entity recognition in English.\n",
        "\n",
        "The data comes from news articles published in October 2025. The articles were downloaded from the Europresse platform.\n",
        "\n",
        "The articles were first annotated using the ```english_web_core_trf``` pipeline from spaCy. The annotation were then corrected by 4 annotators, and exported in a .csv file to be used as data to fine-tune the model.\n",
        "\n",
        "Pre-trained model that was fine-tuned : https://huggingface.co/distilbert/distilbert-base-uncased\n",
        "\n",
        "The model is then evaluated using a confusion matrix. The results are very encouraging.\n",
        "<br></br>\n",
        "*Please note that much of the code used here is either inspired by or directly taken from Ms. Delphine Bernhard's Machine Learning course given at the Universit√© de Strasbourg.*\n",
        "___"
      ],
      "metadata": {
        "id": "Mjzr_dEf8AKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data retrieval and formatting"
      ],
      "metadata": {
        "id": "TJYdhhkDGXrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dataset = Dataset.from_csv(\"/content/correction_annotation - corpus_anno_no_text.csv\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "I5YcWbybIrxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check values to discard\n",
        "dataset.unique(\"correction_finale\")"
      ],
      "metadata": {
        "id": "DKwtQllOGd0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withdraw anotations that were labeled as wrong\n",
        "dataset = dataset.filter(lambda x: (\n",
        "    x[\"correction_finale\"] is not None\n",
        "    and x[\"correction_finale\"] != \" NULL\")\n",
        ")"
      ],
      "metadata": {
        "id": "8TJnMs_AXZnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only useful columns\n",
        "dataset = dataset.remove_columns([col for col in dataset.column_names if col not in [\"expression\", \"correction_finale\"]])\n",
        "dataset"
      ],
      "metadata": {
        "id": "0yunAPiJJQrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set of tags\n",
        "tags = set(dataset['correction_finale'])\n",
        "# encode labels as ClassLabel\n",
        "dataset = dataset.cast_column(\"correction_finale\", ClassLabel(names=list(tags)))\n",
        "dataset = dataset.rename_column(\"correction_finale\", \"label\")"
      ],
      "metadata": {
        "id": "tLGdm0CaVqyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target classes\n",
        "target_classes = dataset.features['label'].names\n",
        "target_classes"
      ],
      "metadata": {
        "id": "UbSOfd4KIa-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical IDs for classes\n",
        "[dataset.features['label'].str2int(c) for c in target_classes]"
      ],
      "metadata": {
        "id": "SQhXkUpJKl6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link between numerical classes and semantic classes\n",
        "label2id = {target_classes[i]:i for i in range(len(target_classes))}\n",
        "id2label = {i:target_classes[i] for i in range(len(target_classes))}"
      ],
      "metadata": {
        "id": "8heWqRoPKrWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first train/test split\n",
        "# shuffle because data is currenlty in chronological order\n",
        "dataset = dataset.train_test_split(test_size = 0.2, shuffle=True, seed=47)\n",
        "# second split to get a validation set as well\n",
        "dataset2 = dataset[\"test\"].train_test_split(test_size = 0.5, shuffle=True, seed = 47)\n",
        "# DatasetDict that contains all splitted data\n",
        "ds = DatasetDict({\n",
        "    \"train\": dataset[\"train\"],\n",
        "    \"validation\": dataset2[\"train\"],\n",
        "    \"test\": dataset2[\"test\"]\n",
        "})"
      ],
      "metadata": {
        "id": "MMZ8SSguLr8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check structure of DatasetDict\n",
        "ds"
      ],
      "metadata": {
        "id": "iMQ9gGiwPkDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'].features"
      ],
      "metadata": {
        "id": "6B4fuA9JL1i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0:3]"
      ],
      "metadata": {
        "id": "WkyymEILCZrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "JXtdLHHLL3_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dBERT = \"distilbert/distilbert-base-uncased\"\n",
        "# load model's pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dBERT)"
      ],
      "metadata": {
        "id": "Qv1HIM8YMq89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize function\n",
        "def preprocess_function(dataset):\n",
        "    return tokenizer(dataset[\"expression\"], padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "-6GdLuNPMrXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data tokenization\n",
        "tokenized_data = ds.map(preprocess_function, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "yjhjcTC4M7Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation settings"
      ],
      "metadata": {
        "id": "rgnmHhS7NHkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation metrics\n",
        "accuracy = evaluate.load('accuracy')\n",
        "f1_metric = evaluate.load(\"f1\")"
      ],
      "metadata": {
        "id": "81oRlsqYNw_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    return {\"accuracy\": acc['accuracy'], \"f1-macro\": f1[\"f1\"]}"
      ],
      "metadata": {
        "id": "C_eGt6sfN5qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning"
      ],
      "metadata": {
        "id": "w3C8ktgGOAOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "# GPU is used if available, if not CPU is used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder_model = AutoModel.from_pretrained(model_dBERT).to(device)"
      ],
      "metadata": {
        "id": "ie1GkiLoOEgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "# training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_dBERT}-finetuned-NER-LoRA\",\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=3e-4,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1-macro\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# LoRA parameters for fine-tuning process\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8, # avant: 4\n",
        "    lora_alpha=32, # avant : 16\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_lin\", \"v_lin\"]\n",
        ")"
      ],
      "metadata": {
        "id": "1TvbojKLOwQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "# early stopping to avoid unnecessary training\n",
        "early_stop = EarlyStoppingCallback(\n",
        "    early_stopping_patience=2,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "def get_model():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_dBERT, num_labels=len(target_classes), id2label=id2label, label2id=label2id\n",
        "    ).to(device)\n",
        "    lora_model = get_peft_model(model, lora_config)\n",
        "    return lora_model\n",
        "\n",
        "def init_trainer():\n",
        "  model = get_model()\n",
        "  return Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=tokenized_data[\"train\"],\n",
        "      eval_dataset=tokenized_data[\"validation\"],\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "      callbacks=[early_stop],\n",
        "      compute_metrics=compute_metrics\n",
        "  ), model"
      ],
      "metadata": {
        "id": "y_cT_1nFOR7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization of training\n",
        "trainer, model = init_trainer()"
      ],
      "metadata": {
        "id": "fxIRVDnCOTeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model fine-tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "QcH_DM4eV03b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance analysis"
      ],
      "metadata": {
        "id": "sT4B7eSLzMw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output = trainer.predict(tokenized_data['validation'])"
      ],
      "metadata": {
        "id": "dti5vsV8XIm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output.metrics"
      ],
      "metadata": {
        "id": "tXS4FWo5XQG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results into json file\n",
        "import json\n",
        "\n",
        "results = {\n",
        "    \"f1\": preds_output.metrics.get(\"test_f1-macro\"),\n",
        "    \"accuracy\": preds_output.metrics.get(\"test_accuracy\"),\n",
        "    \"loss\": preds_output.metrics.get(\"test_loss\"),\n",
        "}\n",
        "\n",
        "with open(\"results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(results, f, indent=2)"
      ],
      "metadata": {
        "id": "-WRMK7wXJ-pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
        "y_valid = tokenized_data['validation']['label']"
      ],
      "metadata": {
        "id": "jiTx5_7FXV5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a confusion matrix to analyze performances qualitatively\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    labels_for_fig = [l[0:4]+'.' for l in labels]\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                  display_labels=labels_for_fig)\n",
        "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "    plt.title(\"Normalized confusion matrix\")\n",
        "    plt.savefig(\"confusion_matrix.png\", dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_preds, y_valid, target_classes)"
      ],
      "metadata": {
        "id": "68QOUiEKXZkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}